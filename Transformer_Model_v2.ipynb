{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rE8MGNBRgmM"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SS6XHuokRbA8"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset,DataLoader\n",
        "from dataclasses import dataclass\n",
        "import torch.nn.functional as F\n",
        "from typing import Union\n",
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import json\n",
        "import math\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptvqauqJcgAp",
        "outputId": "cd058b7a-8879-4da2-b926-4b6d363e57ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EVhnwBNdYiw",
        "outputId": "76e9aca8-5b63-47f0-d069-5c324d9a1e4b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaDLw6DhQCdC"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE=32\n",
        "HIDDEN_SIZE=512*2\n",
        "HEAD_NUMBERS=8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGj4AcJQRfnQ"
      },
      "source": [
        "##Preproccess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wi1KODrLgfA4"
      },
      "outputs": [],
      "source": [
        "def string_preprocess(sen:str):\n",
        "\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHHZYuvLoutE"
      },
      "source": [
        "###Tokinizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJp5ZmY3mN25"
      },
      "outputs": [],
      "source": [
        "class TokinizerUtils():\n",
        "  def __init__(self,max_sequance_length,vocab_size_limit,longets_string_len=None):\n",
        "\n",
        "    self.fit_dataset=[]\n",
        "    self.longets_string_len=longets_string_len\n",
        "\n",
        "    self.max_sequance_length=max_sequance_length\n",
        "    self.vocab_size_limit=vocab_size_limit\n",
        "\n",
        "\n",
        "    if self.max_sequance_length!=None:\n",
        "      self.longets_string_len=self.max_sequance_length\n",
        "\n",
        "  vocabolary=dict({\"PAD\":0,\"UNK\":1,\"MASK\":2,\"SOS\":3,\"EOS\":4})\n",
        "\n",
        "  def _extract_data_from_pandas_csv(self,dataframe_paths:list):\n",
        "    for dataframe_path in dataframe_paths:\n",
        "      dataframe=pd.read_csv(dataframe_path)\n",
        "\n",
        "      _,column_numbers=dataframe.shape\n",
        "      for column_number in range(column_numbers):\n",
        "        column=dataframe.iloc[:,column_number]\n",
        "        [self.fit_dataset.append(str(i)) for i in column]\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return len(self.vocabolary)\n",
        "\n",
        "  def __getitem__(self,index_or_word)->Union[int,str]:\n",
        "    if type(index_or_word)==int:\n",
        "      return self._get_word_from_index(index_or_word)\n",
        "    elif type(index_or_word)==str:\n",
        "      return  self._get_index_from_word(index_or_word)\n",
        "    else:\n",
        "      raise TypeError(\"data type is not suported, make sure its a int or str\")\n",
        "\n",
        "  def _get_word_from_index(self,word):\n",
        "    return list(self.vocabolary.keys())[list(self.vocabolary.values()).index(word)]\n",
        "\n",
        "  def _get_index_from_word(self,idx):\n",
        "    return list(self.vocabolary.keys()).index(idx)\n",
        "\n",
        "  def _fit_vocabalary_on_dataset(self)->None:\n",
        "    self._find_lenght_of_longest_string()\n",
        "\n",
        "  def _find_lenght_of_longest_string(self)->None:\n",
        "    for string in self.fit_dataset:\n",
        "      if self.max_sequance_length==None:\n",
        "        if len(string)>self.longets_string_len:\n",
        "          self.longets_string_len=len(string)\n",
        "\n",
        "  def fit(self)->None:\n",
        "    self._find_lenght_of_longest_string()\n",
        "    vocabolary=self._make_dict_of_dataset()\n",
        "\n",
        "    most_used_words_in_order=self._sort_dict_by_most_word_count(vocabolary)\n",
        "    self._add_words_to_dict_in_order(most_used_words_in_order)\n",
        "\n",
        "\n",
        "  def _make_dict_of_dataset(self)->dict:\n",
        "    vocabolary=dict()\n",
        "    for array in self.fit_dataset:\n",
        "      string_split_to_array=array.split()\n",
        "      for word in string_split_to_array:\n",
        "        if word not in vocabolary:\n",
        "          vocabolary[word]=1\n",
        "        else:\n",
        "          vocabolary[word]+=1\n",
        "    return vocabolary\n",
        "\n",
        "  def _sort_dict_by_most_word_count(self,vocabolary):\n",
        "    return {k: v for k, v in sorted(vocabolary.items(),reverse=True, key=lambda item: item[1])}\n",
        "\n",
        "  def _add_words_to_dict_in_order(self,most_used_words_in_order)->None:\n",
        "    for i in range(self.vocab_size_limit-len(self.vocabolary)):\n",
        "      try:\n",
        "        self.vocabolary[list(most_used_words_in_order.keys())[i]]=5+i\n",
        "      except IndexError:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyBmPszVlGYU"
      },
      "outputs": [],
      "source": [
        "class Tokinizer(TokinizerUtils):\n",
        "  def __init__(self,max_sequance_length,vocab_size_limit,longets_string_len=None):\n",
        "    super().__init__(max_sequance_length,vocab_size_limit,longets_string_len)\n",
        "\n",
        "  def encode(self,string) -> torch.tensor:\n",
        "    encode=[]\n",
        "    encode.append(self.vocabolary[\"SOS\"])\n",
        "    encode=self._encode_string(string,encode)\n",
        "    encode.append(self.vocabolary[\"EOS\"])\n",
        "\n",
        "    return self._pad_encoding(encode)\n",
        "\n",
        "  def _encode_string(self,string,encode)->list:\n",
        "    for word in string.split():\n",
        "      if len(encode)==self.longets_string_len-1:\n",
        "        break\n",
        "      if word not in self.vocabolary.keys():\n",
        "        encode.append(self.vocabolary[\"UNK\"])\n",
        "      else:\n",
        "        encode.append(self.vocabolary[str(word)])\n",
        "    return encode\n",
        "\n",
        "  def _pad_encoding(self,encode):\n",
        "    requredPaddingLength=self.longets_string_len-len(encode)\n",
        "    for pad in range(requredPaddingLength):\n",
        "      encode.append(self.vocabolary[\"PAD\"])\n",
        "    return encode\n",
        "\n",
        "  def decode(self,token_tensor:torch.tensor) -> str:\n",
        "    self._check_tensor_dimension(token_tensor)\n",
        "    return self._decode_tensor(token_tensor)\n",
        "\n",
        "  def _check_tensor_dimension(self,tensor):\n",
        "    tensor_dimensions=len(tensor.shape)\n",
        "    if tensor_dimensions>1:\n",
        "      raise  ValueError(f\"tensor has to many dimensions. expected 1 got {tensor_dimensions}\")\n",
        "\n",
        "  def _decode_tensor(self,tensor):\n",
        "    string_decode=\"\"\n",
        "\n",
        "    for token in tensor.cpu().numpy():\n",
        "      if token==self.vocabolary[\"SOS\"]:\n",
        "        pass\n",
        "      elif token==self.vocabolary[\"EOS\"]:\n",
        "        break\n",
        "      else:\n",
        "       string_decode=\"{} {}\".format(string_decode,list(self.vocabolary.keys())[list(self.vocabolary.values()).index(token)])\n",
        "\n",
        "    return string_decode[1:]\n",
        "\n",
        "\n",
        "tokinizerFitData=pd.read_csv(\"/content/drive/MyDrive/shitpostCommentData.csv\")\n",
        "\n",
        "input=[str(i) for i in tokinizerFitData[\"input\"]]\n",
        "tokinizer=Tokinizer(max_sequance_length=30,vocab_size_limit=6000)\n",
        "tokinizer._extract_data_from_pandas_csv([\"/content/drive/MyDrive/shitpostCommentData.csv\",\"/content/drive/MyDrive/preTrainingData.csv\"])\n",
        "tokinizer.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSA8NVmMoy3i"
      },
      "source": [
        "###Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2koJoEGemkB"
      },
      "outputs": [],
      "source": [
        "class promptDataset(Dataset):\n",
        "  def __init__(self,path:str,tokinizer):\n",
        "    df=pd.read_csv(path)\n",
        "\n",
        "    df_question=[string_preprocess(str(i)) for i in df[\"input\"]]\n",
        "    df_answer=[string_preprocess(str(i)) for i in df[\"target\"]]\n",
        "\n",
        "\n",
        "    self.question_dataset_tensor=torch.tensor([tokinizer.encode(string) for string in df_question])\n",
        "    self.answer_dataset_tensor=torch.tensor([tokinizer.encode(string) for string in df_answer])\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.question_dataset_tensor)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    return self.question_dataset_tensor[idx],self.answer_dataset_tensor[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2q0mD0yo-9u"
      },
      "source": [
        "###Create Datasets/Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmBpARSOrJ5K"
      },
      "outputs": [],
      "source": [
        "def testDataloader(dataloader,sample):\n",
        "  for input,target in dataloader:\n",
        "    print(tokinizer.token_to_string(input[sample]))\n",
        "    print(tokinizer.token_to_string(target[sample]))\n",
        "# testDataloader(pretrainingTestDataloader,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0XSYgtzRvZY"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoKeSvR4Jtlc"
      },
      "source": [
        "###Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vI8ah8wvRwBL"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self ,input_sequance_length:int,head_num, vocabalary_size:int,hidden_size:int) -> None:\n",
        "    super().__init__()\n",
        "    self.sequentialBlock=nn.Sequential(\n",
        "      InputLayer(vocabalary_size,input_sequance_length),\n",
        "      EncoderBlock(input_sequance_length, head_num,  hidden_size, input_sequance_length),\n",
        "      EncoderBlock(input_sequance_length, head_num,  hidden_size, input_sequance_length),\n",
        "      EncoderBlock(input_sequance_length, head_num,  hidden_size, input_sequance_length),\n",
        "      EncoderBlock(input_sequance_length, head_num,  hidden_size, input_sequance_length),\n",
        "      EncoderBlock(input_sequance_length, head_num,  hidden_size, input_sequance_length),\n",
        "\n",
        "      EncoderBlock(input_sequance_length,head_num,  hidden_size, input_sequance_length),\n",
        "      nn.Linear(input_sequance_length,vocabalary_size)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self,input:torch.LongTensor):\n",
        "    # print(\"self.sequentialBlock(input): \",self.sequentialBlock(input).shape)\n",
        "\n",
        "\n",
        "    return self.sequentialBlock(input)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "346ZAXSzJ3HK"
      },
      "source": [
        "###Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r12ZAksnJ2zt"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self ,input_sequance_length:int, head_num,vocabalary_size:int ,hidden_size:int) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    self.inputLayer=InputLayer(vocabalary_size,input_sequance_length)\n",
        "\n",
        "    self.sequential=nn.Sequential(\n",
        "        DecoderBlock(input_sequance_length,head_num,True,hidden_size, input_sequance_length),\n",
        "        DecoderBlock(input_sequance_length,head_num,True,hidden_size, input_sequance_length),\n",
        "        DecoderBlock(input_sequance_length,head_num,True,hidden_size, input_sequance_length),\n",
        "        DecoderBlock(input_sequance_length,head_num,True,hidden_size, input_sequance_length),\n",
        "        DecoderBlock(input_sequance_length,head_num,True,hidden_size, input_sequance_length),\n",
        "\n",
        "        DecoderBlock(input_sequance_length,head_num,False,hidden_size, input_sequance_length),\n",
        "    )\n",
        "\n",
        "    self.reshapeEncoder=nn.Linear(vocabalary_size,input_sequance_length)\n",
        "\n",
        "    self.output=nn.Linear(input_sequance_length,vocabalary_size)\n",
        "\n",
        "  def forward(self,target:torch.LongTensor, encoder_output):\n",
        "    reshapeEncoder=self.reshapeEncoder(encoder_output)\n",
        "    posisonalEmbedding=self.inputLayer(target)\n",
        "\n",
        "    sequential=self.sequential({posisonalEmbedding,reshapeEncoder})\n",
        "\n",
        "    return F.softmax(self.output(sequential),-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHtIixHTT-BB"
      },
      "source": [
        "###Encoder/Decoder block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9TzSOS3jDus"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, input_sequance_length, head_num,  hidden_size, output):\n",
        "    super().__init__()\n",
        "    self.sequentialBlock=nn.Sequential(\n",
        "      # MaskedLanguageModel(input_sequance_length, tokinizer, input_sequance_length),\n",
        "      MultiHeadedAttention(input_sequance_length, head_num, 2, hidden_size, input_sequance_length),\n",
        "      FeedForward(input_sequance_length, hidden_size, output),\n",
        "    )\n",
        "\n",
        "  def forward(self,input):\n",
        "    return self.sequentialBlock(input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5baPUsXH0MA"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self ,input,head_num,return_encoder_output,  hidden_size, output):\n",
        "    super().__init__()\n",
        "    self.return_encoder_output=return_encoder_output\n",
        "\n",
        "\n",
        "    self.maskedMultiHeadedAttention=MultiHeadedAttention(input ,head_num, None, hidden_size,output)\n",
        "\n",
        "    self.multiHeadedAttention=MultiHeadedAttention(input ,head_num, None, hidden_size,output)\n",
        "    self.feedForwardBlock2=FeedForward(input ,hidden_size ,output)\n",
        "\n",
        "  def forward(self,inputData):\n",
        "\n",
        "    input,encoder_output=inputData\n",
        "\n",
        "    maskedMultiHeadedAttention=self.maskedMultiHeadedAttention(input)\n",
        "\n",
        "    multiHeadedAttention=self.multiHeadedAttention(maskedMultiHeadedAttention,encoder_output)\n",
        "    if self.return_encoder_output==True:\n",
        "      return self.feedForwardBlock2(multiHeadedAttention),encoder_output\n",
        "\n",
        "    return self.feedForwardBlock2(multiHeadedAttention)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qd68AqRG4Yd"
      },
      "source": [
        "###Sub layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flNfZ5fXG8Vy"
      },
      "outputs": [],
      "source": [
        "class InputLayer(nn.Module):\n",
        "  def __init__(self,input_size,output_size):\n",
        "    super().__init__()\n",
        "    self.embedding=nn.Embedding(input_size,output_size)\n",
        "    self.posisonalEncoding=nn.Embedding(input_size,output_size)\n",
        "\n",
        "  def forward(self,input):\n",
        "    try:\n",
        "      batch,squanceLength=input.shape\n",
        "    except ValueError:\n",
        "      input=input.unsqueeze(0)\n",
        "    finally:\n",
        "      batch,squanceLength=input.shape\n",
        "\n",
        "    embedding=self.embedding(input)\n",
        "    posisonalEmbedding=self.posisonalEncoding(torch.arange(squanceLength).to(device))\n",
        "\n",
        "    return embedding+posisonalEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwO6F3P0G-If"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size,output_size):\n",
        "    super().__init__()\n",
        "    self.sequentialBlock=nn.Sequential(\n",
        "      nn.Linear(input_size,hidden_size),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(hidden_size,output_size),\n",
        "    )\n",
        "    self.norm=nn.LayerNorm(output_size)\n",
        "\n",
        "\n",
        "  def forward(self,input):\n",
        "    output=self.sequentialBlock(input)\n",
        "    return self.norm(output+input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Nvn7_ufUIVk"
      },
      "source": [
        "###Masked Languange Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8PPudQKs-2Z"
      },
      "outputs": [],
      "source": [
        "class MaskStringVectorWithPorbebility():\n",
        "  def __init__(self, tokinizer, maskToken, wordChanceOfSelection=.15 ,wordChanceOfSwapWithMaskToken=.8, wordChanceOfSwapWithRandomToken=.1, wordChanceOfstayingTheSame=.1):\n",
        "    self.tokinizer=tokinizer\n",
        "    self.maskToken=maskToken\n",
        "    self.wordChanceOfSelection=wordChanceOfSelection\n",
        "    self.wordChanceOfSwapWithMaskToken=wordChanceOfSwapWithMaskToken\n",
        "    self.wordChanceOfSwapWithRandomToken=wordChanceOfSwapWithRandomToken\n",
        "    self.wordChanceOfstayingTheSame=wordChanceOfstayingTheSame\n",
        "\n",
        "  def _batchedDataset(self,inputData):\n",
        "    batchedMaskedDataset=[]\n",
        "    batchSize,SequanceLength=inputData.shape\n",
        "    for batchNumber in range(batchSize):\n",
        "      VectorSequance=inputData[batchNumber]\n",
        "      batchedMaskedDataset.append(self._maskSequanceVector(VectorSequance))\n",
        "\n",
        "    return batchedMaskedDataset\n",
        "\n",
        "  def _maskSequanceVector(self,VectorSequance):\n",
        "    vectorAfterMaksingMechanisam=[]\n",
        "    for idx in range(len(VectorSequance)):\n",
        "      if random.random()<self.wordChanceOfSelection:\n",
        "        token=self._selectWordWithProbAndModify(idx,VectorSequance)\n",
        "        vectorAfterMaksingMechanisam.append(token)\n",
        "      else:\n",
        "        vectorAfterMaksingMechanisam.append(VectorSequance[idx])\n",
        "\n",
        "    return vectorAfterMaksingMechanisam\n",
        "\n",
        "  def _selectWordWithProbAndModify(self,idx,sentanceInVectorForm):\n",
        "    if random.random()<self.wordChanceOfSwapWithMaskToken:\n",
        "      return self.maskToken\n",
        "    elif random.random()<self.wordChanceOfSwapWithRandomToken:\n",
        "      randomWord=random.randrange(len(self.tokinizer))\n",
        "      return randomWord\n",
        "    elif random.random()<self.wordChanceOfstayingTheSame:\n",
        "      return sentanceInVectorForm[idx]\n",
        "    else:\n",
        "      return sentanceInVectorForm[idx]\n",
        "\n",
        "  def maskSentance(self,inputData : torch.tensor):\n",
        "    inputData=inputData.cpu().detach().numpy()\n",
        "    if len(inputData.shape)==2:\n",
        "      return torch.Tensor(self._batchedDataset(inputData))\n",
        "    elif len(inputData.shape)==1:\n",
        "      return torch.Tensor(self._maskSequanceVector(inputData))\n",
        "    else:\n",
        "      raise ValueError(f\"expected data batch size to be 1D or 2D but resived {len(inputData.shape)}D\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2TiCPLbUH6z"
      },
      "outputs": [],
      "source": [
        "class MaskedLanguageModel(nn.Module):\n",
        "  def __init__(self,input,tokinizer,output):\n",
        "    super().__init__()\n",
        "    self.maskingMechanism=MaskStringVectorWithPorbebility(tokinizer,2)\n",
        "    self.tokenEmbedding=nn.Embedding(input,output)\n",
        "    self.positonalEmbedding=nn.Embedding(input,output)\n",
        "    self.languageEmbedding=nn.Embedding(input,output)\n",
        "\n",
        "  def forward(self,input):\n",
        "    wordPorbilityToWord=input.argmax(-1)\n",
        "    inputAfterMaskedMechanism=self.maskingMechanism.maskSentance(wordPorbilityToWord)\n",
        "\n",
        "\n",
        "    tokenEmbedding=self.tokenEmbedding(wordPorbilityToWord)\n",
        "    positonalEmbedding=self.positonalEmbedding(wordPorbilityToWord)\n",
        "    languageEmbedding=self.languageEmbedding(wordPorbilityToWord)\n",
        "\n",
        "    embeddings=languageEmbedding+positonalEmbedding+tokenEmbedding\n",
        "\n",
        "\n",
        "    return F.softmax(embeddings,-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4QvobyATYxn"
      },
      "source": [
        "###Attention Mechanism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ci6An5HWHwem"
      },
      "outputs": [],
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "  def __init__(self,input_size,head_num,mask,hidden_size, output_size):\n",
        "    super().__init__()\n",
        "    self.attention=AttentionMechanism(input_size,head_num,mask,hidden_size, output_size)\n",
        "    self.norm=nn.LayerNorm(output_size)\n",
        "\n",
        "  def forward(self,input,encoder_output=None):\n",
        "    if encoder_output!=None:\n",
        "      attn=self.attention(input,encoder_output)\n",
        "    else:\n",
        "      attn=self.attention(input)\n",
        "\n",
        "    output=self.norm(input+attn)\n",
        "\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPD6NdVSTYfY"
      },
      "outputs": [],
      "source": [
        "class AttentionMechanism(nn.Module):\n",
        "  def __init__(self,input_size,head_num,mask,hidden_size, output_size):\n",
        "    super().__init__()\n",
        "    self.head_num=head_num\n",
        "    self.hidden_size=hidden_size\n",
        "    self.mask=mask\n",
        "\n",
        "    self.query=nn.Linear(input_size,hidden_size)\n",
        "    self.key=nn.Linear(input_size,hidden_size)\n",
        "    self.value=nn.Linear(input_size,hidden_size)\n",
        "\n",
        "    self.output=nn.Linear(hidden_size,output_size)\n",
        "\n",
        "\n",
        "  def forward(self,input,encoder_output=None):\n",
        "    batch_size,seq_len,_=input.shape\n",
        "\n",
        "\n",
        "    if encoder_output!=None:\n",
        "      query=self.query(input)\n",
        "      key=self.key(encoder_output)\n",
        "      value=self.value(encoder_output)\n",
        "\n",
        "    else:\n",
        "      query=self.query(input)\n",
        "      key=self.key(input)\n",
        "      value=self.value(input)\n",
        "\n",
        "\n",
        "\n",
        "    query=query.view(batch_size, seq_len, self.head_num, self.hidden_size // self.head_num).transpose(1, 2)\n",
        "    key=key.view(batch_size, seq_len, self.head_num, self.hidden_size // self.head_num).transpose(1, 2)\n",
        "    value=value.view(batch_size, seq_len, self.head_num, self.hidden_size // self.head_num).transpose(1, 2)\n",
        "\n",
        "    attn_value=self.calculateScaledDotProduct(query,key,value)\n",
        "\n",
        "    attn_weight=attn_value.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_size)\n",
        "\n",
        "\n",
        "    return self.output(attn_weight)\n",
        "\n",
        "\n",
        "  def calculateScaledDotProduct(self,query,key,value):\n",
        "    scores=self.calculateVectorSimilaritis(query.transpose(2, 3),key)\n",
        "    dk=key.shape[-1]\n",
        "    scaled_attention_logits=scores/dk\n",
        "    if self.mask!=None:\n",
        "      scaled_attention_logits += (2 * -1e9)\n",
        "    attn_wight=F.softmax(scaled_attention_logits, dim=-1)\n",
        "\n",
        "    return torch.matmul(attn_wight,value.transpose(2, 3))\n",
        "\n",
        "  def calculateVectorSimilaritis(self,query,key):\n",
        "    return torch.matmul(query,key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvW8-l5pJ7Q4"
      },
      "source": [
        "###Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfSo_651J68V"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, device, target_vocab_size):\n",
        "    super().__init__()\n",
        "    self.encoder=encoder\n",
        "    self.decoder=decoder\n",
        "    self.device=device\n",
        "    self.target_vocab_size=target_vocab_size\n",
        "\n",
        "  def forward(self,input:torch.LongTensor, target:torch.LongTensor,softmax=False):\n",
        "    encoderOutput=self.encoder(input)\n",
        "\n",
        "    decoderOutput=self.decoder(target,encoderOutput)\n",
        "\n",
        "    if softmax==True:\n",
        "      return F.softmax(decoderOutput,-1)\n",
        "    return decoderOutput\n",
        "\n",
        "  def generate_tokens(self, input, start_token, end_token, max_length=512):\n",
        "    batch_size, seq_len=input.shape\n",
        "\n",
        "    encoder_output=self.encoder(input)\n",
        "\n",
        "    target_sequence=torch.tensor([[[0]*5910]*max_length]).long().to(device)\n",
        "    target_batch,target_seq_len,_=target_sequence.shape\n",
        "\n",
        "    for batch in range(target_batch):\n",
        "      for token_index in range(target_seq_len):\n",
        "\n",
        "        decoder_output=self.decoder(target_sequence.argmax(-1),encoder_output)\n",
        "\n",
        "        next_token_probs=F.softmax(decoder_output,-1)\n",
        "        next_token_index = torch.multinomial(next_token_probs[:, -1, :], num_samples=5910)\n",
        "\n",
        "\n",
        "\n",
        "        target_sequence[batch][token_index]=next_token_index.long().to(device)\n",
        "        if next_token_index.argmax(-1)==end_token:\n",
        "          break\n",
        "    return target_sequence\n",
        "\n",
        "\n",
        "TOKINIZER_VOCAB=len(tokinizer)\n",
        "\n",
        "encoder=Encoder(tokinizer.longets_string_len, HEAD_NUMBERS, TOKINIZER_VOCAB, HIDDEN_SIZE).to(device)\n",
        "# encoder.load_state_dict(torch.load(\"/content/drive/MyDrive/preTrainedencoder_2.pth\",map_location=device))\n",
        "\n",
        "decoder=Decoder(tokinizer.longets_string_len, HEAD_NUMBERS, TOKINIZER_VOCAB, HIDDEN_SIZE).to(device)\n",
        "\n",
        "seq2seq=Seq2Seq(encoder,decoder,device,TOKINIZER_VOCAB).to(device)\n",
        "# seq2seq.load_state_dict(torch.load(\"/content/drive/MyDrive/preTrainedTransformer_2.pth\",map_location=device))\n",
        "# seq2seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwCUSL_qOgQ1",
        "outputId": "ef971111-4383-494b-9069-dff153250ec3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 30])\n",
            "smartest algonquian snoovatar intelligence borat yellow redirect puerto cyberpunk public indexing spun updated redditgifts Ssself block left. miracle woods now, usually coming constitution maya estimate campaign sam identify screen expectancy\n"
          ]
        }
      ],
      "source": [
        "def make_prediction(string:str):\n",
        "  input_sequence=tokinizer.encode(string)\n",
        "  input_sequence=torch.tensor([input_sequence])\n",
        "  generated_tokens = seq2seq.generate_tokens(input_sequence.to(device), 3, 4,tokinizer.longets_string_len).argmax(-1)\n",
        "  print(generated_tokens.shape)\n",
        "  return tokinizer.decode(generated_tokens.squeeze(0).cpu())\n",
        "pred=make_prediction(\"adolf\")\n",
        "print(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cloiNHlC7HYP"
      },
      "source": [
        "##parameter count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quxGAE6RpbGt",
        "outputId": "3b95bca7-28e1-4b19-bf77-4a17216fcb04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The seq2seq model has 4,272,354 trainable parameters\n",
            "The encoder model has 1,669,386 trainable parameters\n",
            "The decoder model has 2,602,968 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def parametersCount(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The seq2seq model has {parametersCount(seq2seq):,} trainable parameters')\n",
        "print(f'The encoder model has {parametersCount(encoder):,} trainable parameters')\n",
        "print(f'The decoder model has {parametersCount(decoder):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyAQe4SPRp1p"
      },
      "source": [
        "##Optimizer & Loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKBSvWwOR2Ea"
      },
      "outputs": [],
      "source": [
        "optimizer=torch.optim.Adam(seq2seq.parameters(),lr=0.001)\n",
        "loss=nn.CrossEntropyLoss(ignore_index=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFIDYe3hR5yi"
      },
      "source": [
        "##Training Loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf3ROVt3kquw"
      },
      "source": [
        "###Training Util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYxFoDLvfvCU"
      },
      "outputs": [],
      "source": [
        "class TrainingUtil():\n",
        "  def __init__(self, EPOCHS ,model,loss ,device ,savePath ,tokinizer, csvFilePath, testSplit):\n",
        "    self.EPOCHS=EPOCHS\n",
        "    self.model=model\n",
        "    self.loss=loss\n",
        "    self.device=device\n",
        "    self.savePath=savePath\n",
        "    self.tokinizer=tokinizer\n",
        "\n",
        "    self.train_dataloader,self.test_dataloader=self.createDatasetFromPandasCsv(csvFilePath,testSplit)\n",
        "\n",
        "    self.currentEpoch=0\n",
        "    self.startPreTraining()\n",
        "\n",
        "  def accuracy(self,predictions,targets):\n",
        "    assert predictions.shape == targets.shape, \"Shapes of predictions and targets must match.\"\n",
        "\n",
        "    num_correct = (predictions == targets).sum().item()\n",
        "\n",
        "    total_samples = targets.numel()\n",
        "    accuracy_value = num_correct / total_samples\n",
        "    return accuracy_value*100\n",
        "\n",
        "  def getLossAndAccuracy(self,prediction,target):\n",
        "    prediction=prediction.to(self.device)\n",
        "    target=target.to(self.device).type(torch.int64)\n",
        "\n",
        "    prediction_loss=self.loss(prediction.view(-1,prediction.shape[-1]),target.view(-1))\n",
        "    prediction_acc=self.accuracy(prediction.argmax(2),target)\n",
        "\n",
        "    return prediction_loss,prediction_acc\n",
        "\n",
        "  def createDatasetFromPandasCsv(self,csvFilePath,testSplit):\n",
        "    dataset=promptDataset(csvFilePath,tokinizer)\n",
        "    print(len(dataset))\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    trainDataset, testDataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    train_dataloader=DataLoader(trainDataset,batch_size=BATCH_SIZE,shuffle=True)\n",
        "    test_dataloader=DataLoader(testDataset,batch_size=BATCH_SIZE,shuffle=True)\n",
        "\n",
        "    return train_dataloader, test_dataloader\n",
        "\n",
        "  def make_prediction(self,input,target=None)->float:\n",
        "    input=input.to(self.device)\n",
        "    if target==None:\n",
        "      return self.model(input)\n",
        "    else:\n",
        "      target=target.to(self.device)\n",
        "      return self.model(input,target)\n",
        "\n",
        "\n",
        "  def startPreTraining(self)->None:\n",
        "    epochsToRun=self.EPOCHS+1\n",
        "    for epoch in tqdm(range(1,epochsToRun)):\n",
        "      self.currentEpoch=epoch\n",
        "\n",
        "      train_state=self.trainingLoop()\n",
        "      train_state=next(iter(train_state))\n",
        "      train_loss,train_acc=train_state[0],train_state[1]\n",
        "\n",
        "      test_state=self.testingLoop()\n",
        "      test_state=next(iter(test_state))\n",
        "      test_loss,test_acc=test_state[0],test_state[1]\n",
        "\n",
        "      torch.save(self.model.state_dict(), f\"/content/drive/MyDrive/{self.savePath}.pth\")\n",
        "      print(f\"\\n epoch: {epoch} | train_loss: {train_loss:.2f}, train_acc: {train_acc:.1f}% | test_loss: {test_loss:.2f}, test_acc: {test_acc:.1f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njOtCCcWkxi4"
      },
      "source": [
        "###Training seq2seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Qrx44RHR5bt"
      },
      "outputs": [],
      "source": [
        "class TrainNN(TrainingUtil):\n",
        "  def __init__(self,EPOCHS ,model,loss ,device ,savePath ,tokinizer, csvFilePath, testSplit):\n",
        "    super().__init__(EPOCHS ,model,loss ,device ,savePath ,tokinizer, csvFilePath, testSplit)\n",
        "\n",
        "  def trainingLoop(self):\n",
        "    self.model.train()\n",
        "    for input,target in self.train_dataloader:\n",
        "      optimizer.zero_grad()\n",
        "      # print(\"\\ninput: \",input[0])\n",
        "      # print(\"target: \",target[0])\n",
        "      prediction=self.make_prediction(input,target)\n",
        "\n",
        "      train_loss,train_acc=self.getLossAndAccuracy(prediction,target)\n",
        "\n",
        "      train_loss.backward()\n",
        "      optimizer.step()\n",
        "      yield train_loss,train_acc\n",
        "\n",
        "  def testingLoop(self):\n",
        "    self.model.eval()\n",
        "    with torch.inference_mode():\n",
        "      for input,target in self.test_dataloader:\n",
        "\n",
        "        prediction=self.make_prediction(input,target)\n",
        "        test_loss,test_acc=self.getLossAndAccuracy(prediction,target)\n",
        "        if self.currentEpoch%10 == 0:\n",
        "          print(\"\\ninput: \",tokinizer.decode(input[0]))\n",
        "          print(\"\\ntarget: \",tokinizer.decode(target[0]))\n",
        "          print(\"\\nprediction: \",tokinizer.decode(prediction[0].argmax(-1)))\n",
        "        yield test_loss,test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8CvEFfID3TH"
      },
      "source": [
        "###Training Encocer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBm4OHjdD0s2"
      },
      "outputs": [],
      "source": [
        "class TrainEncoder(TrainingUtil):\n",
        "  def __init__(self, EPOCHS ,model,loss ,device ,savePath ,tokinizer, csvFilePath, testSplit):\n",
        "    super().__init__(EPOCHS ,model,loss ,device ,savePath ,tokinizer, csvFilePath, testSplit)\n",
        "\n",
        "  def trainingLoop(self):\n",
        "    self.model.train()\n",
        "    for input,target in self.train_dataloader:\n",
        "      optimizer.zero_grad()\n",
        "      prediction=self.make_prediction(input).type(torch.float32)\n",
        "      train_loss,train_acc=self.getLossAndAccuracy(prediction,input)\n",
        "\n",
        "      train_loss.backward()\n",
        "      optimizer.step()\n",
        "      yield train_loss,train_acc\n",
        "\n",
        "  def testingLoop(self):\n",
        "    self.model.eval()\n",
        "    with torch.inference_mode():\n",
        "      for input,target in self.test_dataloader:\n",
        "\n",
        "        prediction=self.make_prediction(input).type(torch.float32)\n",
        "        test_loss,test_acc=self.getLossAndAccuracy(prediction,input)\n",
        "\n",
        "        if self.currentEpoch%10 == 0:\n",
        "          print(\"\\ninput: \",tokinizer.decode(input[0]))\n",
        "          print(\"\\nprediction: \",tokinizer.decode(prediction[0].argmax(-1)))\n",
        "\n",
        "        yield test_loss,test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rweJlzBNk-ty"
      },
      "source": [
        "###Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cfvoh4cd3eLv"
      },
      "outputs": [],
      "source": [
        "EPOCHS=1500\n",
        "pretrain=\"encoder\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 980
        },
        "id": "y6NJz5rnPOQJ",
        "outputId": "af1a5b87-c3b7-49e3-ea95-2ba7d2b0bebe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25690\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/1500 [00:01<25:56,  1.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " epoch: 1 | train_loss: 8.87, train_acc: 0.0% | test_loss: 8.66, test_acc: 0.0%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 2/1500 [00:02<25:50,  1.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " epoch: 2 | train_loss: 8.66, train_acc: 0.0% | test_loss: 8.47, test_acc: 2.7%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 3/1500 [00:03<30:37,  1.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " epoch: 3 | train_loss: 8.57, train_acc: 2.4% | test_loss: 8.39, test_acc: 2.8%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 4/1500 [00:05<34:17,  1.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " epoch: 4 | train_loss: 8.44, train_acc: 3.3% | test_loss: 8.31, test_acc: 3.1%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 5/1500 [00:06<35:46,  1.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " epoch: 5 | train_loss: 8.31, train_acc: 2.9% | test_loss: 8.19, test_acc: 7.2%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 6/1500 [00:08<37:34,  1.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " epoch: 6 | train_loss: 8.20, train_acc: 6.8% | test_loss: 8.10, test_acc: 14.9%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 7/1500 [00:09<36:39,  1.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " epoch: 7 | train_loss: 8.14, train_acc: 13.0% | test_loss: 8.10, test_acc: 12.6%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 8/1500 [00:10<33:17,  1.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " epoch: 8 | train_loss: 8.06, train_acc: 15.3% | test_loss: 8.00, test_acc: 16.0%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 9/1500 [00:11<31:43,  1.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " epoch: 9 | train_loss: 8.02, train_acc: 15.2% | test_loss: 7.96, test_acc: 17.0%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 10/1500 [00:13<31:09,  1.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "input:  party came for retaining her father as her campaign manager after his arrest on child sexual abuse charges she was\n",
            "\n",
            "prediction:  yalta yalta sexuality months the heat months the coordinate and anti and this, and foul months the cultivating corresponds granted\n",
            "\n",
            " epoch: 10 | train_loss: 7.94, train_acc: 16.5% | test_loss: 7.89, test_acc: 17.4%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 11/1500 [00:14<30:59,  1.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " epoch: 11 | train_loss: 7.95, train_acc: 19.0% | test_loss: 7.87, test_acc: 19.1%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 12/1500 [00:15<30:32,  1.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " epoch: 12 | train_loss: 7.87, train_acc: 18.4% | test_loss: 7.84, test_acc: 19.2%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 13/1500 [00:16<29:08,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " epoch: 13 | train_loss: 7.81, train_acc: 19.9% | test_loss: 7.74, test_acc: 20.3%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 14/1500 [00:17<31:25,  1.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " epoch: 14 | train_loss: 7.83, train_acc: 18.1% | test_loss: 7.78, test_acc: 20.6%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-fb751e554887>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpretrain\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"encoder\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mTrainEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msavePath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"preTrainedEncoder_2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokinizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokinizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcsvFilePath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/preTrainingData.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestSplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mpretrain\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"seq2seq\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mTrainNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq2seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msavePath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"preTrainedSeq2seq_2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokinizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokinizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcsvFilePath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/preTrainingData.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestSplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-c66d03773d7d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, EPOCHS, model, loss, device, savePath, tokinizer, csvFilePath, testSplit)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTrainEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainingUtil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0msavePath\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mtokinizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsvFilePath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestSplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0msavePath\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mtokinizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsvFilePath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestSplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrainingLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-1db4028333d9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, EPOCHS, model, loss, device, savePath, tokinizer, csvFilePath, testSplit)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentEpoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartPreTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-1db4028333d9>\u001b[0m in \u001b[0;36mstartPreTraining\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m       \u001b[0mtrain_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainingLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m       \u001b[0mtrain_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m       \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-c66d03773d7d>\u001b[0m in \u001b[0;36mtrainingLoop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       \u001b[0mprediction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLossAndAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-1db4028333d9>\u001b[0m in \u001b[0;36mmake_prediction\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-25e343b927ad>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequentialBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-45b59479fa45>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequentialBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-6a2607f902be>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, encoder_output)\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mattn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mattn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-bbb2aa1b25c6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, encoder_output)\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if pretrain==\"encoder\":\n",
        "  TrainEncoder(EPOCHS=EPOCHS,model=encoder,loss=loss,device=device,savePath=\"preTrainedEncoder_2\",tokinizer=tokinizer,csvFilePath=\"/content/drive/MyDrive/preTrainingData.csv\",testSplit=.8)\n",
        "elif pretrain==\"seq2seq\":\n",
        "  TrainNN(EPOCHS=EPOCHS,model=seq2seq,loss=loss,device=device,savePath=\"preTrainedSeq2seq_2\",tokinizer=tokinizer,csvFilePath=\"/content/drive/MyDrive/preTrainingData.csv\",testSplit=.8)\n",
        "else:\n",
        "  TrainNN(EPOCHS=EPOCHS,model=seq2seq,loss=loss,device=device,savePath=\"trainedSeq2seq_2\",tokinizer=tokinizer,csvFilePath=\"/content/drive/MyDrive/shitpostCommentData.csv\",testSplit=.8)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "YoKeSvR4Jtlc",
        "346ZAXSzJ3HK",
        "Lf3ROVt3kquw",
        "njOtCCcWkxi4"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
