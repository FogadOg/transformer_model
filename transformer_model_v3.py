# -*- coding: utf-8 -*-
"""Transformer Model v3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hpaHWE0JZ5mWhqn-npW0Js8KG-dwwriF

##Test1 encoder

*   encoder blocks = 8
*   hidden size = 512
*   batch size = 64
*   number of attention heads = 8
*   loss = crossEntropyLoss
*   optimizer = adam
*   lr = .001
*   epochs = 500
####i notcing a critical error in may code, where the seq2seq model was outputing a tensor with the shape of batchSize*sequanceLength*sequanceLength insted of batchSize*sequanceLength*vocabalaryLength i managed to fix it. and after that i tried to per train the encoder to understand the context of words, i tok the input sequance and picked 15% of the sequance and with a probability of 80% i swaped the token with a masked token, with a probebility of 10% i swaped it with a random token and made it predict the masked tokens. but the encoder reaches a max avarage accuracy of 36% and loss a of 2.3

##Test2 encoder
*   encoder blocks = 6
*   hidden size = 512
*   batch size = 32
*   number of attention heads = 8
*   loss = crossEntropyLoss
*   optimizer = adam
*   lr = .001
*   epochs = 1000
####i started to hypotisese that the transformer bad proformance was due to the multi headed attention. and after changing some lines of code i thought migth be cause the problem due to their unreadebility, i managed to get the encoders accuracy to 71% accuracy and .20 loss but i had to drop the masked language model because it was keeping the accuracy at 19%. but i will fix it after i make sure everything is running smothely

##Test3 encoder
*   encoder blocks = 6
*   hidden size = 512
*   batch size = 64
*   number of attention heads = 8
*   loss = crossEntropyLoss
*   optimizer = adam
*   lr = .001
*   epochs = 3000
####after furder modifications to the encoder model i managed to get the accuracy to 95% and the loss to .50. the reason for the encoders poor proformance was because of the pre training data which was structured in a bad way, but still the current models accuracy of 95% is missleading as it struggels to predict the next word, it keeps defulting to predict "the" as the next word rarely predicting somthing else.

##Imports
"""

from torch.utils.data import Dataset,DataLoader
from dataclasses import dataclass
import torch.nn.functional as F
from typing import Union
from tqdm import tqdm
from torch import nn
import pandas as pd
import numpy as np
import random
import string
import torch
import json
import math
import csv
import re

from google.colab import drive
drive.mount('/content/drive')

device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

BATCH_SIZE=64
HIDDEN_SIZE=64
HEAD_NUMBERS=11
HEAD_EMBED=33

"""##Preproccess Data"""

def string_preprocess(sen:str):
  sentence = re.sub(r"\s+[a-zA-Z]\s+", ' ', sen)
  sentence= re.sub(r'[^\w\s]', ' ', sentence)
  sentence = re.sub(r'\s+', ' ', sentence)

  return sentence.lower()

"""###Tokinizer"""

class TokinizerUtils():
  def __init__(self,vocab_size_limit,max_sequance_length):

    self.fit_dataset=[]
    self.longets_string_len=0

    self.max_sequance_length=max_sequance_length
    self.vocab_size_limit=vocab_size_limit


    if self.max_sequance_length!=None:
      self.longets_string_len=self.max_sequance_length

  vocabolary=dict({"PAD":0,"UNK":1,"MASK":2,"SOS":3,"EOS":4})

  def _extract_data_from_pandas_csv(self,dataframe_paths:list):
    for dataframe_path in dataframe_paths:
      dataframe=pd.read_csv(dataframe_path)

      _,column_numbers=dataframe.shape
      for column_number in range(column_numbers):
        column=dataframe.iloc[:,column_number]
        [self.fit_dataset.append(string_preprocess(str(i))) for i in column]

  def __len__(self) -> int:
    return len(self.vocabolary)

  def __getitem__(self,index_or_word)->Union[int,str]:
    if type(index_or_word)==int:
      return self._get_word_from_index(index_or_word)
    elif type(index_or_word)==str:
      return  self._get_index_from_word(index_or_word)
    else:
      raise TypeError("data type is not suported, make sure its a int or str")

  def _get_word_from_index(self,word):
    return list(self.vocabolary.keys())[list(self.vocabolary.values()).index(word)]

  def _get_index_from_word(self,idx):
    return list(self.vocabolary.keys()).index(idx)

  def _fit_vocabalary_on_dataset(self)->None:
    self._find_lenght_of_longest_string()

  def _find_lenght_of_longest_string(self)->None:
    for string in self.fit_dataset:
      if self.max_sequance_length==None:
        if len(string)>self.longets_string_len:
          self.longets_string_len=len(string)

  def fit(self)->None:
    self._find_lenght_of_longest_string()
    vocabolary=self._make_dict_of_dataset()

    most_used_words_in_order=self._sort_dict_by_most_word_count(vocabolary)
    self._add_words_to_dict_in_order(most_used_words_in_order)

  def _make_dict_of_dataset(self)->dict:
    vocabolary=dict()
    for array in self.fit_dataset:
      string_split_to_array=array.split()
      for word in string_split_to_array:
        if word not in vocabolary:
          vocabolary[word]=1
        else:
          vocabolary[word]+=1
    return vocabolary

  def _sort_dict_by_most_word_count(self,vocabolary):
    return {k: v for k, v in sorted(vocabolary.items(),reverse=True, key=lambda item: item[1])}

  def _add_words_to_dict_in_order(self,most_used_words_in_order)->None:
    for i in range(self.vocab_size_limit-len(self.vocabolary)):
      try:
        self.vocabolary[list(most_used_words_in_order.keys())[i]]=5+i
      except IndexError:
        break

class Tokinizer(TokinizerUtils):
  def __init__(self,vocab_size_limit,max_sequance_length=None):
    super().__init__(vocab_size_limit,max_sequance_length)

  def encode(self,string) -> torch.tensor:
    encode=[]
    encode.append(self.vocabolary["SOS"])
    encode=self._encode_string(string,encode)
    encode.append(self.vocabolary["EOS"])

    return self._pad_encoding(encode)

  def _encode_string(self,string,encode)->list:
    for word in string.split():
      if len(encode)==self.longets_string_len-1:
        break
      if word not in self.vocabolary.keys():
        encode.append(self.vocabolary["UNK"])
      else:
        encode.append(self.vocabolary[str(word)])
    return encode

  def _pad_encoding(self,encode):
    requredPaddingLength=self.longets_string_len-len(encode)
    for pad in range(requredPaddingLength):
      encode.append(self.vocabolary["PAD"])
    return encode

  def decode(self,token_tensor:torch.tensor) -> str:
    self._check_tensor_dimension(token_tensor)
    return self._decode_tensor(token_tensor)

  def _check_tensor_dimension(self,tensor):
    tensor_dimensions=len(tensor.shape)
    if tensor_dimensions>1:
      raise  ValueError(f"tensor has to many dimensions. expected 1d got {tensor_dimensions}d")

  def _decode_tensor(self,tensor):
    string_decode=""

    for token in tensor.cpu().numpy():
      if token==self.vocabolary["SOS"]:
        pass
      elif token==self.vocabolary["EOS"]:
        break
      else:
       string_decode="{} {}".format(string_decode,list(self.vocabolary.keys())[list(self.vocabolary.values()).index(token)])

    return string_decode[1:]


tokinizer=Tokinizer(max_sequance_length=33,vocab_size_limit=60000)
tokinizer._extract_data_from_pandas_csv(["/content/drive/MyDrive/shitpostCommentData.csv","/content/drive/MyDrive/preTrainingData.csv"])
tokinizer.fit()

"""###Dataset"""

class promptDataset(Dataset):
  def __init__(self,path:str,tokinizer):
    df=pd.read_csv(path)

    df_question=[string_preprocess(str(i)) for i in df["input"]]
    df_answer=[string_preprocess(str(i)) for i in df["target"]]


    self.question_dataset_tensor=torch.tensor([tokinizer.encode(string) for string in df_question])
    self.answer_dataset_tensor=torch.tensor([tokinizer.encode(string) for string in df_answer])


  def __len__(self):
    return len(self.question_dataset_tensor)

  def __getitem__(self,idx):
    return self.question_dataset_tensor[idx],self.answer_dataset_tensor[idx]

"""##Model"""

class PrintLayer(nn.Module):
    def __init__(self,layerName):
      self.layerName=layerName
      super(PrintLayer, self).__init__()

    def forward(self, x):
      print(self.layerName,x.shape)
      return x

"""###Encoder"""

class Encoder(nn.Module):
  def __init__(self ,input_sequance_length,  vocabalary_size, hidden_size) -> None:
    super().__init__()
    self.sequentialBlock=nn.Sequential(
      InputLayer(vocabalary_size,input_sequance_length),
      EncoderBlock(input_sequance_length, vocabalary_size, hidden_size, input_sequance_length),
      EncoderBlock(input_sequance_length, vocabalary_size, hidden_size, input_sequance_length),
      EncoderBlock(input_sequance_length, vocabalary_size, hidden_size, input_sequance_length),

      nn.Linear(input_sequance_length,vocabalary_size)
    )


  def forward(self,input:torch.LongTensor):
    # print("self.sequentialBlock(input): ",self.sequentialBlock(input).shape)


    return self.sequentialBlock(input)

"""###Decoder"""

class Decoder(nn.Module):
  def __init__(self ,input_sequance_length,vocabalary_size ,hidden_size) -> None:
    super().__init__()

    self.inputLayer=InputLayer(vocabalary_size,input_sequance_length)

    self.sequential=nn.Sequential(
        DecoderBlock(input_sequance_length,vocabalary_size,True,hidden_size, input_sequance_length),
        DecoderBlock(input_sequance_length,vocabalary_size,True,hidden_size, input_sequance_length),
        DecoderBlock(input_sequance_length,vocabalary_size,True,hidden_size, input_sequance_length),
        DecoderBlock(input_sequance_length,vocabalary_size,True,hidden_size, input_sequance_length),
        DecoderBlock(input_sequance_length,vocabalary_size,True,hidden_size, input_sequance_length),

        DecoderBlock(input_sequance_length,vocabalary_size,False,hidden_size, input_sequance_length),
    )

    self.reshapeEncoder=nn.Linear(vocabalary_size,input_sequance_length)

    self.output=nn.Linear(input_sequance_length,vocabalary_size)

  def forward(self,target:torch.LongTensor, encoder_output):
    reshapeEncoder=self.reshapeEncoder(encoder_output)
    posisonalEmbedding=self.inputLayer(target)

    sequential=self.sequential({posisonalEmbedding,reshapeEncoder})

    return F.softmax(self.output(sequential),-1)

"""###Encoder/Decoder block"""

class EncoderBlock(nn.Module):
  def __init__(self, input_sequance_length,vocabalary_size,  hidden_size, output):
    super().__init__()
    self.sequentialBlock=nn.Sequential(
      MultiHeadedAttention(vocabalary_size, input_sequance_length),
      FeedForward(input_sequance_length, hidden_size, output),
    )

  def forward(self,input):
    return self.sequentialBlock(input)

class DecoderBlock(nn.Module):
  def __init__(self ,input_sequance_length,vocabalary_size,return_encoder_output,  hidden_size, output):
    super().__init__()
    self.return_encoder_output=return_encoder_output


    self.maskedMultiHeadedAttention=MultiHeadedAttention(vocabalary_size, input_sequance_length)

    self.multiHeadedAttention=MultiHeadedAttention(vocabalary_size,input_sequance_length)
    self.feedForwardBlock=FeedForward(input_sequance_length, hidden_size ,output)

  def forward(self,inputData):

    input,encoder_output=inputData

    maskedMultiHeadedAttention=self.maskedMultiHeadedAttention(input)

    multiHeadedAttention=self.multiHeadedAttention(maskedMultiHeadedAttention,encoder_output)


    if self.return_encoder_output==True:
      return self.feedForwardBlock(multiHeadedAttention),encoder_output

    return self.feedForwardBlock(multiHeadedAttention)

"""###Sub layers"""

class InputLayer(nn.Module):
  def __init__(self,input_size,output_size):
    super().__init__()
    self.embedding=nn.Embedding(input_size,output_size)
    self.posisonalEncoding=nn.Embedding(input_size,output_size)

  def forward(self,input):
    try:
      batch,squanceLength=input.shape
    except ValueError:
      input=input.unsqueeze(0)
    finally:
      batch,squanceLength=input.shape

    embedding=self.embedding(input)
    posisonalEmbedding=self.posisonalEncoding(torch.arange(squanceLength).to(device))

    return embedding+posisonalEmbedding

class FeedForward(nn.Module):
  def __init__(self,input_size,hidden_size,output_size):
    super().__init__()
    self.sequentialBlock=nn.Sequential(
      nn.Linear(input_size,hidden_size),
      nn.ReLU(),
      nn.Linear(hidden_size,output_size),
    )
    self.norm=nn.LayerNorm(output_size)


  def forward(self,input):

    output=self.sequentialBlock(input)

    return self.norm(output+input)

"""###Masked Languange Model"""

class MaskStringVectorWithPorbebility():
  def __init__(self, tokinizer, maskToken, wordChanceOfSelection=.15 ,wordChanceOfSwapWithMaskToken=.8, wordChanceOfSwapWithRandomToken=.1, wordChanceOfstayingTheSame=.1):
    self.tokinizer=tokinizer
    self.maskToken=maskToken
    self.wordChanceOfSelection=wordChanceOfSelection
    self.wordChanceOfSwapWithMaskToken=wordChanceOfSwapWithMaskToken
    self.wordChanceOfSwapWithRandomToken=wordChanceOfSwapWithRandomToken
    self.wordChanceOfstayingTheSame=wordChanceOfstayingTheSame

  def _batchedDataset(self,inputData):
    batchedMaskedDataset=[]
    batchSize,SequanceLength=inputData.shape
    for batchNumber in range(batchSize):
      VectorSequance=inputData[batchNumber]
      batchedMaskedDataset.append(self._maskSequanceVector(VectorSequance))

    return batchedMaskedDataset

  def _maskSequanceVector(self,VectorSequance):
    vectorAfterMaksingMechanisam=[]
    for idx in range(len(VectorSequance)):
      if random.random()<self.wordChanceOfSelection:
        token=self._selectWordWithProbAndModify(idx,VectorSequance)
        vectorAfterMaksingMechanisam.append(token)
      else:
        vectorAfterMaksingMechanisam.append(VectorSequance[idx])

    return vectorAfterMaksingMechanisam

  def _selectWordWithProbAndModify(self,idx,sentanceInVectorForm):
    if random.random()<self.wordChanceOfSwapWithMaskToken:
      return self.maskToken
    elif random.random()<self.wordChanceOfSwapWithRandomToken:
      randomWord=random.randrange(len(self.tokinizer))
      return randomWord
    elif random.random()<self.wordChanceOfstayingTheSame:
      return sentanceInVectorForm[idx]
    else:
      return sentanceInVectorForm[idx]

  def maskSentance(self,inputData : torch.tensor):
    inputData=inputData.cpu().detach().numpy()
    if len(inputData.shape)==2:
      return torch.Tensor(self._batchedDataset(inputData))
    elif len(inputData.shape)==1:
      return torch.Tensor(self._maskSequanceVector(inputData))
    else:
      raise ValueError(f"expected data batch size to be 1D or 2D but resived {len(inputData.shape)}D")

class MaskedLanguageModel(nn.Module):
  def __init__(self,input,tokinizer,output):
    super().__init__()
    self.maskingMechanism=MaskStringVectorWithPorbebility(tokinizer,2)
    self.tokenEmbedding=nn.Embedding(input,output)
    self.positonalEmbedding=nn.Embedding(input,output)
    self.languageEmbedding=nn.Embedding(input,output)

  def forward(self,input):
    wordPorbilityToWord=input.argmax(-1)
    inputAfterMaskedMechanism=self.maskingMechanism.maskSentance(wordPorbilityToWord)


    tokenEmbedding=self.tokenEmbedding(wordPorbilityToWord)
    positonalEmbedding=self.positonalEmbedding(wordPorbilityToWord)
    languageEmbedding=self.languageEmbedding(wordPorbilityToWord)

    embeddings=languageEmbedding+positonalEmbedding+tokenEmbedding


    return F.softmax(embeddings,-1)

"""###Attention Mechanism"""

class MultiHeadedAttention(nn.Module):
  def __init__(self,vocabalary_size, output_size):
    super().__init__()
    self.attention=nn.MultiheadAttention(tokinizer.longets_string_len,HEAD_NUMBERS)
    self.norm=nn.LayerNorm(output_size)

  def forward(self,input,encoder_output=None):

    if encoder_output!=None:
      attn, _=self.attention(input,encoder_output,encoder_output)
    else:

      attn, _=self.attention(input,input,input)
    # print("attention: ",attn)
    output=self.norm(input+attn)

    return output

"""###Seq2Seq"""

class Seq2Seq(nn.Module):
  def __init__(self, encoder, decoder, device, target_vocab_size):
    super().__init__()
    self.encoder=encoder
    self.decoder=decoder
    self.device=device
    self.target_vocab_size=target_vocab_size

  def forward(self,input:torch.LongTensor, target:torch.LongTensor,softmax=False):
    encoderOutput=self.encoder(input)

    decoderOutput=self.decoder(target,encoderOutput)

    if softmax==True:
      return F.softmax(decoderOutput,2)
    return decoderOutput

  def generate_tokens(self, input, start_token, end_token, max_length=512):
    batch_size, seq_len=input.shape

    encoder_output=self.encoder(input)

    target_sequence=torch.tensor([[[0]*self.target_vocab_size]*max_length]).long().to(device)
    target_batch,target_seq_len,_=target_sequence.shape

    for batch in range(target_batch):
      for token_index in range(target_seq_len):

        decoder_output=self.decoder(target_sequence.argmax(-1),encoder_output)

        next_token_probs=F.softmax(decoder_output,-1)
        next_token_index = torch.multinomial(next_token_probs[:, -1, :], num_samples=self.target_vocab_size)



        target_sequence[batch][token_index]=next_token_index.long().to(device)
        if next_token_index.argmax(-1)==end_token:
          break
    return target_sequence


TOKINIZER_VOCAB=len(tokinizer)

encoder=Encoder(tokinizer.longets_string_len, TOKINIZER_VOCAB, HIDDEN_SIZE).to(device)
# encoder.load_state_dict(torch.load("/content/drive/MyDrive/preTrainedEncoder_2.pth",map_location=device))

decoder=Decoder(tokinizer.longets_string_len, TOKINIZER_VOCAB, HIDDEN_SIZE).to(device)

seq2seq=Seq2Seq(encoder,decoder,device,TOKINIZER_VOCAB).to(device)
# seq2seq.load_state_dict(torch.load("/content/drive/MyDrive/preTrainedTransformer_2.pth",map_location=device))
# seq2seq

def make_prediction(string:str):
  input_sequence=tokinizer.encode(string)
  input_sequence=torch.tensor([input_sequence])
  generated_tokens = seq2seq.generate_tokens(input_sequence.to(device), 3, 4,tokinizer.longets_string_len).argmax(-1)
  return tokinizer.decode(generated_tokens.squeeze(0).cpu())
pred=make_prediction("olla")
print(pred)

"""##parameter count"""

def parametersCount(model):
  return sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f'The seq2seq model has {parametersCount(seq2seq):,} trainable parameters')
print(f'The encoder model has {parametersCount(encoder):,} trainable parameters')
print(f'The decoder model has {parametersCount(decoder):,} trainable parameters')

"""##Optimizer & Loss

"""

optimizer=torch.optim.Adam(seq2seq.parameters(),lr=0.001)
# loss=nn.CrossEntropyLoss(ignore_index=0)
loss=nn.CrossEntropyLoss()

"""##Training Loops

###Training Util
"""

class TrainingUtil():
  def __init__(self, EPOCHS ,model,loss ,device ,savePath ,tokinizer, csvFilePath, trainSplit):
    self.EPOCHS=EPOCHS
    self.model=model
    self.loss=loss
    self.device=device
    self.savePath=savePath
    self.tokinizer=tokinizer
    self.train_dataloader,self.test_dataloader=self.createDatasetFromPandasCsv(csvFilePath,trainSplit)

    self.currentEpoch=0
    self.startPreTraining()

  def accuracy(self,predictions,targets):
    assert predictions.shape == targets.shape, "Shapes of predictions and targets must match."

    num_correct = (predictions == targets).sum().item()

    total_samples = targets.numel()
    accuracy_value = num_correct / total_samples
    return accuracy_value*100

  def getLossAndAccuracy(self, prediction, target):
    prediction = prediction.to(self.device)
    target = target.to(self.device).long()

    print("prediction: ", prediction.shape)
    print("target: ", target.shape)

    prediction_loss = self.loss(prediction.view(-1, prediction.shape[-1]), target.view(-1))
    prediction_acc = self.accuracy(prediction.argmax(2), target)

    return prediction_loss, prediction_acc

  def createDatasetFromPandasCsv(self,csvFilePath,trainSplit):
    dataset=promptDataset(csvFilePath,tokinizer)
    print("dataset size: ",len(dataset))
    train_size = int(0.8 * len(dataset))
    test_size = len(dataset) - train_size
    trainDataset, testDataset = torch.utils.data.random_split(dataset, [train_size, test_size])

    train_dataloader=DataLoader(trainDataset,batch_size=BATCH_SIZE,shuffle=True)
    test_dataloader=DataLoader(testDataset,batch_size=BATCH_SIZE,shuffle=True)

    return train_dataloader, test_dataloader

  def make_prediction(self,input,target=None)->float:
    input=input.to(self.device)
    if target==None:
      return self.model(input)
    else:
      target=target.to(self.device)
      return self.model(input,target)


  def startPreTraining(self)->None:
    epochsToRun=self.EPOCHS+1
    for epoch in tqdm(range(1,epochsToRun)):
      self.currentEpoch=epoch

      train_state=self.trainingLoop()
      train_state=next(iter(train_state))
      train_loss,train_acc=train_state[0],train_state[1]

      test_state=self.testingLoop()
      test_state=next(iter(test_state))
      test_loss,test_acc=test_state[0],test_state[1]

      torch.save(self.model.state_dict(), f"/content/drive/MyDrive/{self.savePath}.pth")
      print(f"\n epoch: {epoch} | train_loss: {train_loss:.2f}, train_acc: {train_acc:.1f}% | test_loss: {test_loss:.2f}, test_acc: {test_acc:.1f}%")

"""###Training seq2seq"""

class TrainNN(TrainingUtil):
    def __init__(self, EPOCHS, model, loss, device, savePath, tokinizer, csvFilePath, trainSplit):
        super().__init__(EPOCHS, model, loss, device, savePath, tokinizer, csvFilePath, trainSplit)

    def trainingLoop(self):
        self.model.train()
        for input, target in self.train_dataloader:
            input = input.to(self.device)
            target = target.to(self.device)

            prediction = self.make_prediction(input, target)
            train_loss, train_acc = self.getLossAndAccuracy(prediction, target)

            if self.currentEpoch % 10 == 0:
              print("loss: ", train_loss.item())
              print(f"input:{input.dtype}, target:{target.dtype}, prediction:{prediction.dtype}")
              print(f"input: requires_grad={input.requires_grad}, target: requires_grad={target.requires_grad}, prediction: requires_grad={prediction.requires_grad}")
              print(f"input: grad={input.grad}, target: grad={target.grad}, prediction: grad={prediction.grad}")
              for name, param in self.model.named_parameters():
                print(f'{name}: requires_grad={param.requires_grad}, dtype={param.dtype}, grad={param.grad.shape}')

            train_loss.backward()
            optimizer.step()
            yield train_loss, train_acc

    def testingLoop(self):
        self.model.eval()
        with torch.no_grad():
            for input, target in self.test_dataloader:
                input = input.to(self.device)
                target = target.to(self.device)

                prediction = self.make_prediction(input, target)
                test_loss, test_acc = self.getLossAndAccuracy(prediction, target)

                if self.currentEpoch % 10 == 0:
                  print("-------------------------------------------------test---------------------------------------------------------")
                  print("\ninput: ", self.tokinizer.decode(input[0]))
                  print("\ntarget: ", self.tokinizer.decode(target[0]))
                  print("\nprediction: ", self.tokinizer.decode(prediction[0].argmax(-1)))

                  # print("loss: ", test_loss.item())
                  # print(f"input:{input.dtype}, target:{target.dtype}, prediction:{prediction.dtype}")
                  # print(f"input: requires_grad={input.requires_grad}, target: requires_grad={target.requires_grad}, prediction: requires_grad={prediction.requires_grad}")
                  # print(f"input: grad={input.grad}, target: grad={target.grad}, prediction: grad={prediction.grad}")
                  # for name, param in self.model.named_parameters():
                  #     print(f'{name}: requires_grad={param.requires_grad}, dtype={param.dtype}, grad={param.grad.shape}')

                yield test_loss, test_acc


TrainNN(EPOCHS=1000, model=seq2seq, loss=loss, device=device, savePath="trainedSeq2seq_2", tokinizer=tokinizer, csvFilePath="/content/drive/MyDrive/shitpostCommentData.csv", trainSplit=0.8)

"""###Training Encoder"""

class TrainEncoder(TrainingUtil):
  def __init__(self, EPOCHS ,model,loss ,device ,savePath ,tokinizer, csvFilePath, trainSplit):
    super().__init__(EPOCHS ,model,loss ,device ,savePath ,tokinizer, csvFilePath, trainSplit)

  def trainingLoop(self):
    self.model.train()
    for input,target in self.train_dataloader:
      optimizer.zero_grad()
      prediction=self.make_prediction(input).type(torch.float32)
      train_loss,train_acc=self.getLossAndAccuracy(prediction,target)

      train_loss.backward()
      optimizer.step()
      yield train_loss,train_acc

  def testingLoop(self):
    self.model.eval()
    with torch.inference_mode():
      for input,target in self.test_dataloader:

        prediction=self.make_prediction(input).type(torch.float32)
        test_loss,test_acc=self.getLossAndAccuracy(prediction,target)

        if self.currentEpoch%10 == 0:
          print("\ninput:      ",tokinizer.decode(input[0]))
          print("\ntarget:     ",tokinizer.decode(target[0]))
          print("\nprediction: ",tokinizer.decode(prediction[0].argmax(-1)))

        yield test_loss,test_acc

"""###Start Training"""

EPOCHS=21
pretrain=None

if pretrain=="encoder":
  TrainEncoder(EPOCHS=EPOCHS,model=encoder,loss=loss,device=device,savePath="preTrainedEncoder_2",tokinizer=tokinizer,csvFilePath="/content/drive/MyDrive/preTrainingData.csv",trainSplit=.8)
else:
  TrainNN(EPOCHS=EPOCHS,model=seq2seq,loss=loss,device=device,savePath="trainedSeq2seq_2",tokinizer=tokinizer,csvFilePath="/content/drive/MyDrive/shitpostCommentData.csv",trainSplit=.8)

line="spez"
for _ in range(10):
  print(len(line.split(" ")))
  print(line)
  with torch.inference_mode():
    line=make_prediction(line)