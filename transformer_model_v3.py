# -*- coding: utf-8 -*-
"""Transformer Model v3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hpaHWE0JZ5mWhqn-npW0Js8KG-dwwriF

##Test1 encoder

*   encoder blocks = 8
*   hidden size = 512
*   batch size = 64
*   number of attention heads = 8
*   loss = crossEntropyLoss
*   optimizer = adam
*   lr = .001
*   epochs = 500
####i notcing a critical error in may code, where the seq2seq model was outputing a tensor with the shape of batchSize*sequanceLength*sequanceLength insted of batchSize*sequanceLength*vocabalaryLength i managed to fix it. and after that i tried to per train the encoder to understand the context of words, i tok the input sequance and picked 15% of the sequance and with a probability of 80% i swaped the token with a masked token, with a probebility of 10% i swaped it with a random token and made it predict the masked tokens. but the encoder reaches a max avarage accuracy of 36% and loss a of 2.3

##Test2 encoder
*   encoder blocks = 6
*   hidden size = 512
*   batch size = 32
*   number of attention heads = 8
*   loss = crossEntropyLoss
*   optimizer = adam
*   lr = .001
*   epochs = 1000
####i started to hypotisese that the transformer bad proformance was due to the multi headed attention. and after changing some lines of code i thought migth be cause the problem due to their unreadebility, i managed to get the encoders accuracy to 71% accuracy and .20 loss but i had to drop the masked language model because it was keeping the accuracy at 19%. but i will fix it after i make sure everything is running smothely

##Test3 encoder
*   encoder blocks = 6
*   hidden size = 512
*   batch size = 64
*   number of attention heads = 8
*   loss = crossEntropyLoss
*   optimizer = adam
*   lr = .001
*   epochs = 3000
####after furder modifications to the encoder model i managed to get the accuracy to 95% and the loss to .50. the reason for the encoders poor proformance was because of the pre training data which was structured in a bad way, but still the current models accuracy of 95% is missleading as it struggels to predict the next word, it keeps defulting to predict "the" as the next word rarely predicting somthing else.

##Imports
"""

from torch.utils.data import Dataset,DataLoader
from dataclasses import dataclass
import torch.nn.functional as F
from typing import Union
from tqdm import tqdm
from torch import nn
import pandas as pd
import numpy as np
import random
import string
import torch
import json
import math
import csv
import re

from google.colab import drive
drive.mount('/content/drive')

device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

BATCH_SIZE=64
HIDDEN_SIZE=64
HEAD_NUMBERS=11
HEAD_EMBED=33

"""##Preproccess Data"""

def string_preprocess(sen:str):
  sentence = re.sub(r"\s+[a-zA-Z]\s+", ' ', sen)
  sentence= re.sub(r'[^\w\s]', ' ', sentence)
  sentence = re.sub(r'\s+', ' ', sentence)

  return sentence.lower()

"""###Tokinizer"""

class TokinizerUtil():
  def __init__(self, maxSquanceLength=33, maxVocabalaryLength=10000):
    self.dictonary={"PAD": 0, "UNK": 1}
    self.maxVocabalaryLength=maxVocabalaryLength
    self.maxSquanceLength=maxSquanceLength

  def __len__(self):
    return len(self.dictonary)

  def addWord(self, word:str) -> None:
    if self.maxVocabalaryLength>len(self.dictonary):
      if word not in self.dictonary:
        self.dictonary[word]=len(self.dictonary)

  def findWordsToken(self, vocabalary:dict, word:str) -> int:
    if word in vocabalary:
      return vocabalary[word]
    return vocabalary["UNK"]

  def pad(self, message:list[int]):
    paddingRequired=self.maxSquanceLength-len(message)
    paddingList=[self.dictonary["PAD"]]*paddingRequired

    return message+paddingList

  def findTokensWord(self, vocabalary:dict, token:int) -> str:
    if token in vocabalary:
      return vocabalary[token]
    return vocabalary[1]



class Tokinizer(TokinizerUtil):
  def encode(self, message:str):
    encodedMessage=[]

    for word in message.split(" "):
      if len(encodedMessage)>=self.maxSquanceLength:
        return self.pad(encodedMessage)

      self.addWord(word)
      token=self.findWordsToken(self.dictonary, word)
      encodedMessage.append(token)

    return self.pad(encodedMessage)

  def decode(self, encodedMessage:list[int]):
    decodedMessage=""
    decodeHash={v: k for k, v in self.dictonary.items()}

    for token in encodedMessage:
      word=self.findTokensWord(decodeHash, token)
      decodedMessage+=word+" "

    return decodedMessage


    def __len__(self):
      return len(self.dictonary)
tokinizer=Tokinizer()

"""###Dataset"""

class promptDataset(Dataset):
  def __init__(self,path:str, tokinizer:Tokinizer):
    df=pd.read_csv(path)

    df_question=[string_preprocess(str(i)) for i in df["input"]]
    df_answer=[string_preprocess(str(i)) for i in df["target"]]


    self.question_dataset_tensor=torch.tensor([tokinizer.encode(string) for string in df_question])
    self.answer_dataset_tensor=torch.tensor([tokinizer.encode(string) for string in df_answer])


  def __len__(self):
    return len(self.question_dataset_tensor)

  def __getitem__(self,idx):
    return self.question_dataset_tensor[idx],self.answer_dataset_tensor[idx]

"""##Model"""

class PrintLayer(nn.Module):
    def __init__(self,layerName):
      self.layerName=layerName
      super(PrintLayer, self).__init__()

    def forward(self, x):
      print(self.layerName,x.shape)
      return x

"""###Encoder"""

class Encoder(nn.Module):
  def __init__(self ,input_sequance_length,  vocabalary_size, hidden_size) -> None:
    super().__init__()
    self.sequentialBlock=nn.Sequential(
      InputLayer(vocabalary_size,input_sequance_length),
      EncoderBlock(input_sequance_length, vocabalary_size, hidden_size, input_sequance_length),
      EncoderBlock(input_sequance_length, vocabalary_size, hidden_size, input_sequance_length),
      EncoderBlock(input_sequance_length, vocabalary_size, hidden_size, input_sequance_length),

      nn.Linear(input_sequance_length,vocabalary_size)
    )


  def forward(self,input:torch.LongTensor):
    # print("self.sequentialBlock(input): ",self.sequentialBlock(input).shape)


    return self.sequentialBlock(input)

"""###Decoder"""

class Decoder(nn.Module):
  def __init__(self ,input_sequance_length,vocabalary_size ,hidden_size) -> None:
    super().__init__()

    self.inputLayer=InputLayer(vocabalary_size,input_sequance_length)

    self.sequential=nn.Sequential(
        DecoderBlock(input_sequance_length,vocabalary_size,True,hidden_size, input_sequance_length),
        DecoderBlock(input_sequance_length,vocabalary_size,True,hidden_size, input_sequance_length),
        DecoderBlock(input_sequance_length,vocabalary_size,True,hidden_size, input_sequance_length),
        DecoderBlock(input_sequance_length,vocabalary_size,True,hidden_size, input_sequance_length),
        DecoderBlock(input_sequance_length,vocabalary_size,True,hidden_size, input_sequance_length),

        DecoderBlock(input_sequance_length,vocabalary_size,False,hidden_size, input_sequance_length),
    )

    self.reshapeEncoder=nn.Linear(vocabalary_size,input_sequance_length)

    self.output=nn.Linear(input_sequance_length,vocabalary_size)

  def forward(self,target:torch.LongTensor, encoder_output):
    reshapeEncoder=self.reshapeEncoder(encoder_output)
    posisonalEmbedding=self.inputLayer(target)

    sequential=self.sequential({posisonalEmbedding,reshapeEncoder})

    return F.softmax(self.output(sequential),-1)

"""###Encoder/Decoder block"""

class EncoderBlock(nn.Module):
  def __init__(self, input_sequance_length,vocabalary_size,  hidden_size, output):
    super().__init__()
    self.sequentialBlock=nn.Sequential(
      MultiHeadedAttention(vocabalary_size, input_sequance_length),
      FeedForward(input_sequance_length, hidden_size, output),
    )

  def forward(self,input):
    return self.sequentialBlock(input)

class DecoderBlock(nn.Module):
  def __init__(self ,input_sequance_length,vocabalary_size,return_encoder_output,  hidden_size, output):
    super().__init__()
    self.return_encoder_output=return_encoder_output


    self.maskedMultiHeadedAttention=MultiHeadedAttention(vocabalary_size, input_sequance_length)

    self.multiHeadedAttention=MultiHeadedAttention(vocabalary_size,input_sequance_length)
    self.feedForwardBlock=FeedForward(input_sequance_length, hidden_size ,output)

  def forward(self,inputData):

    input,encoder_output=inputData

    maskedMultiHeadedAttention=self.maskedMultiHeadedAttention(input)

    multiHeadedAttention=self.multiHeadedAttention(maskedMultiHeadedAttention,encoder_output)


    if self.return_encoder_output==True:
      return self.feedForwardBlock(multiHeadedAttention),encoder_output

    return self.feedForwardBlock(multiHeadedAttention)

"""###Sub layers"""

class InputLayer(nn.Module):
  def __init__(self,input_size,output_size):
    super().__init__()
    self.embedding=nn.Embedding(input_size,output_size)
    self.posisonalEncoding=nn.Embedding(input_size,output_size)

  def forward(self,input):
    try:
      batch,squanceLength=input.shape
    except ValueError:
      input=input.unsqueeze(0)
    finally:
      batch,squanceLength=input.shape

    embedding=self.embedding(input)
    posisonalEmbedding=self.posisonalEncoding(torch.arange(squanceLength).to(device))

    return embedding+posisonalEmbedding

class FeedForward(nn.Module):
  def __init__(self,input_size,hidden_size,output_size):
    super().__init__()
    self.sequentialBlock=nn.Sequential(
      nn.Linear(input_size,hidden_size),
      nn.ReLU(),
      nn.Linear(hidden_size,output_size),
    )
    self.norm=nn.LayerNorm(output_size)


  def forward(self,input):

    output=self.sequentialBlock(input)

    return self.norm(output+input)

"""###Masked Languange Model"""

class MaskStringVectorWithPorbebility():
  def __init__(self, tokinizer, maskToken, wordChanceOfSelection=.15 ,wordChanceOfSwapWithMaskToken=.8, wordChanceOfSwapWithRandomToken=.1, wordChanceOfstayingTheSame=.1):
    self.tokinizer=tokinizer
    self.maskToken=maskToken
    self.wordChanceOfSelection=wordChanceOfSelection
    self.wordChanceOfSwapWithMaskToken=wordChanceOfSwapWithMaskToken
    self.wordChanceOfSwapWithRandomToken=wordChanceOfSwapWithRandomToken
    self.wordChanceOfstayingTheSame=wordChanceOfstayingTheSame

  def _batchedDataset(self,inputData):
    batchedMaskedDataset=[]
    batchSize,SequanceLength=inputData.shape
    for batchNumber in range(batchSize):
      VectorSequance=inputData[batchNumber]
      batchedMaskedDataset.append(self._maskSequanceVector(VectorSequance))

    return batchedMaskedDataset

  def _maskSequanceVector(self,VectorSequance):
    vectorAfterMaksingMechanisam=[]
    for idx in range(len(VectorSequance)):
      if random.random()<self.wordChanceOfSelection:
        token=self._selectWordWithProbAndModify(idx,VectorSequance)
        vectorAfterMaksingMechanisam.append(token)
      else:
        vectorAfterMaksingMechanisam.append(VectorSequance[idx])

    return vectorAfterMaksingMechanisam

  def _selectWordWithProbAndModify(self,idx,sentanceInVectorForm):
    if random.random()<self.wordChanceOfSwapWithMaskToken:
      return self.maskToken
    elif random.random()<self.wordChanceOfSwapWithRandomToken:
      randomWord=random.randrange(len(self.tokinizer))
      return randomWord
    elif random.random()<self.wordChanceOfstayingTheSame:
      return sentanceInVectorForm[idx]
    else:
      return sentanceInVectorForm[idx]

  def maskSentance(self,inputData : torch.tensor):
    inputData=inputData.cpu().detach().numpy()
    if len(inputData.shape)==2:
      return torch.Tensor(self._batchedDataset(inputData))
    elif len(inputData.shape)==1:
      return torch.Tensor(self._maskSequanceVector(inputData))
    else:
      raise ValueError(f"expected data batch size to be 1D or 2D but resived {len(inputData.shape)}D")

class MaskedLanguageModel(nn.Module):
  def __init__(self,input,tokinizer,output):
    super().__init__()
    self.maskingMechanism=MaskStringVectorWithPorbebility(tokinizer,2)
    self.tokenEmbedding=nn.Embedding(input,output)
    self.positonalEmbedding=nn.Embedding(input,output)
    self.languageEmbedding=nn.Embedding(input,output)

  def forward(self,input):
    wordPorbilityToWord=input.argmax(-1)
    inputAfterMaskedMechanism=self.maskingMechanism.maskSentance(wordPorbilityToWord)


    tokenEmbedding=self.tokenEmbedding(wordPorbilityToWord)
    positonalEmbedding=self.positonalEmbedding(wordPorbilityToWord)
    languageEmbedding=self.languageEmbedding(wordPorbilityToWord)

    embeddings=languageEmbedding+positonalEmbedding+tokenEmbedding


    return F.softmax(embeddings,-1)

"""###Attention Mechanism"""

class MultiHeadedAttention(nn.Module):
  def __init__(self,vocabalary_size, output_size):
    super().__init__()
    self.attention=nn.MultiheadAttention(tokinizer.maxSquanceLength,HEAD_NUMBERS)
    self.norm=nn.LayerNorm(output_size)

  def forward(self,input,encoder_output=None):

    if encoder_output!=None:
      attn, _=self.attention(input,encoder_output,encoder_output)
    else:

      attn, _=self.attention(input,input,input)
    # print("attention: ",attn)
    output=self.norm(input+attn)

    return output

"""###Seq2Seq"""

class Seq2Seq(nn.Module):
  def __init__(self, encoder, decoder, device, target_vocab_size):
    super().__init__()
    self.encoder=encoder
    self.decoder=decoder
    self.device=device
    self.target_vocab_size=target_vocab_size

  def forward(self,input:torch.LongTensor, target:torch.LongTensor,softmax=False):
    encoderOutput=self.encoder(input)

    decoderOutput=self.decoder(target,encoderOutput)

    if softmax==True:
      return F.softmax(decoderOutput,2)
    return decoderOutput

  def generate_tokens(self, input, start_token, end_token, max_length=512):
    batch_size, seq_len=input.shape

    encoder_output=self.encoder(input)

    target_sequence=torch.tensor([[[0]*self.target_vocab_size]*max_length]).long().to(device)
    target_batch,target_seq_len,_=target_sequence.shape

    for batch in range(target_batch):
      for token_index in range(target_seq_len):

        decoder_output=self.decoder(target_sequence.argmax(-1),encoder_output)

        next_token_probs=F.softmax(decoder_output,-1)
        next_token_index = torch.multinomial(next_token_probs[:, -1, :], num_samples=self.target_vocab_size)



        target_sequence[batch][token_index]=next_token_index.long().to(device)
        if next_token_index.argmax(-1)==end_token:
          break
    return target_sequence


TOKINIZER_VOCAB=len(tokinizer)

encoder=Encoder(tokinizer.maxSquanceLength, TOKINIZER_VOCAB, HIDDEN_SIZE).to(device)
# encoder.load_state_dict(torch.load("/content/drive/MyDrive/preTrainedEncoder_2.pth",map_location=device))

decoder=Decoder(tokinizer.maxSquanceLength, TOKINIZER_VOCAB, HIDDEN_SIZE).to(device)

seq2seq=Seq2Seq(encoder,decoder,device,TOKINIZER_VOCAB).to(device)
# seq2seq.load_state_dict(torch.load("/content/drive/MyDrive/preTrainedTransformer_2.pth",map_location=device))
# seq2seq

def make_prediction(string:str):
  input_sequence=tokinizer.encode(string)
  input_sequence=torch.tensor([input_sequence])
  generated_tokens = seq2seq.generate_tokens(input_sequence.to(device), 3, 4,tokinizer.maxSquanceLength).argmax(-1)
  return tokinizer.decode(generated_tokens.squeeze(0).cpu())
pred=make_prediction("olla")
print(pred)

"""##parameter count"""

def parametersCount(model):
  return sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f'The seq2seq model has {parametersCount(seq2seq):,} trainable parameters')
print(f'The encoder model has {parametersCount(encoder):,} trainable parameters')
print(f'The decoder model has {parametersCount(decoder):,} trainable parameters')

"""##Optimizer & Loss

"""

optimizer=torch.optim.Adam(seq2seq.parameters(),lr=0.001)
loss=nn.CrossEntropyLoss(ignore_index=0)

"""##Training Loops

###Training Util
"""

class TrainingUtil():
  def __init__(self, EPOCHS ,model,loss ,device ,savePath ,tokinizer, csvFilePath, trainSplit):
    self.EPOCHS=EPOCHS
    self.model=model
    self.loss=loss
    self.device=device
    self.savePath=savePath
    self.tokinizer=tokinizer
    self.train_dataloader,self.test_dataloader=self.createDatasetFromPandasCsv(csvFilePath,trainSplit)

    self.currentEpoch=0
    self.startPreTraining()

  def accuracy(self,predictions,targets):
    assert predictions.shape == targets.shape, "Shapes of predictions and targets must match."

    num_correct = (predictions == targets).sum().item()

    total_samples = targets.numel()
    accuracy_value = num_correct / total_samples
    return accuracy_value*100

  def getLossAndAccuracy(self, prediction, target):
    prediction = prediction.to(self.device)
    target = target.to(self.device).long()

    prediction_loss = self.loss(prediction.view(-1, prediction.shape[-1]), target.view(-1))
    prediction_acc = self.accuracy(prediction.argmax(2), target)

    return prediction_loss, prediction_acc

  def createDatasetFromPandasCsv(self,csvFilePath,trainSplit):
    dataset=promptDataset(csvFilePath,tokinizer)
    print("dataset size: ",len(dataset))
    train_size = int(trainSplit * len(dataset))
    test_size = len(dataset) - train_size
    trainDataset, testDataset = torch.utils.data.random_split(dataset, [train_size, test_size])

    train_dataloader=DataLoader(trainDataset,batch_size=BATCH_SIZE,shuffle=True)
    test_dataloader=DataLoader(testDataset,batch_size=BATCH_SIZE,shuffle=True)

    return train_dataloader, test_dataloader

  def make_prediction(self,input,target=None)->float:
    input=input.to(self.device)
    if target==None:
      return self.model(input)
    else:
      target=target.to(self.device)
      return self.model(input,target)


  def startPreTraining(self)->None:
    epochsToRun=self.EPOCHS+1
    for epoch in tqdm(range(1,epochsToRun)):
      self.currentEpoch=epoch

      train_state=self.trainingLoop()
      train_state=next(iter(train_state))
      train_loss,train_acc=train_state[0],train_state[1]

      test_state=self.testingLoop()
      test_state=next(iter(test_state))
      test_loss,test_acc=test_state[0],test_state[1]

      torch.save(self.model.state_dict(), f"/content/drive/MyDrive/{self.savePath}.pth")
      print(f"\n epoch: {epoch} | train_loss: {train_loss:.2f}, train_acc: {train_acc:.1f}% | test_loss: {test_loss:.2f}, test_acc: {test_acc:.1f}%")

"""###Training seq2seq"""

class TrainNN(TrainingUtil):
    def __init__(self, EPOCHS, model, loss, device, savePath, tokinizer, csvFilePath, trainSplit):
        super().__init__(EPOCHS, model, loss, device, savePath, tokinizer, csvFilePath, trainSplit)

    def trainingLoop(self):
        self.model.train()
        for input, target in self.train_dataloader:
            input = input.to(self.device)
            target = target.to(self.device)

            prediction = self.make_prediction(input, target)
            train_loss, train_acc = self.getLossAndAccuracy(prediction, target)

            # if self.currentEpoch % 10 == 0:
            #     print("loss: ", train_loss.item())
            #     print(f"input:{input.dtype}, target:{target.dtype}, prediction:{prediction.dtype}")
            #     print(f"input: requires_grad={input.requires_grad}, target: requires_grad={target.requires_grad}, prediction: requires_grad={prediction.requires_grad}")
            #     print(f"input: grad={input.grad}, target: grad={target.grad}, prediction: grad={prediction.grad}")
            #     for name, param in self.model.named_parameters():
            #         print(f'{name}: requires_grad={param.requires_grad}, dtype={param.dtype}, grad={param.grad.shape}')

            train_loss.backward()
            optimizer.step()
            yield train_loss, train_acc

    def testingLoop(self):
        self.model.eval()
        with torch.no_grad():
            for input, target in self.test_dataloader:
                input = input.to(self.device)
                target = target.to(self.device)

                prediction = self.make_prediction(input, target)
                test_loss, test_acc = self.getLossAndAccuracy(prediction, target)

                if self.currentEpoch % 10 == 0:
                  print("-------------------------------------------------test---------------------------------------------------------")
                  print("\ninput: ", self.tokinizer.decode(input[0]))
                  print("\ntarget: ", self.tokinizer.decode(target[0]))
                  print("\nprediction: ", self.tokinizer.decode(prediction[0].argmax(-1)))

                  # print("loss: ", test_loss.item())
                  # print(f"input:{input.dtype}, target:{target.dtype}, prediction:{prediction.dtype}")
                  # print(f"input: requires_grad={input.requires_grad}, target: requires_grad={target.requires_grad}, prediction: requires_grad={prediction.requires_grad}")
                  # print(f"input: grad={input.grad}, target: grad={target.grad}, prediction: grad={prediction.grad}")
                  # for name, param in self.model.named_parameters():
                  #     print(f'{name}: requires_grad={param.requires_grad}, dtype={param.dtype}, grad={param.grad.shape}')

                yield test_loss, test_acc

"""###Training Encoder"""

class TrainEncoder(TrainingUtil):
  def __init__(self, EPOCHS ,model,loss ,device ,savePath ,tokinizer, csvFilePath, trainSplit):
    super().__init__(EPOCHS ,model,loss ,device ,savePath ,tokinizer, csvFilePath, trainSplit)

  def trainingLoop(self):
    self.model.train()
    for input,target in self.train_dataloader:
      optimizer.zero_grad()
      prediction=self.make_prediction(input).type(torch.float32)
      train_loss,train_acc=self.getLossAndAccuracy(prediction,target)

      train_loss.backward()
      optimizer.step()
      yield train_loss,train_acc

  def testingLoop(self):
    self.model.eval()
    with torch.inference_mode():
      for input,target in self.test_dataloader:

        prediction=self.make_prediction(input).type(torch.float32)
        test_loss,test_acc=self.getLossAndAccuracy(prediction,target)

        if self.currentEpoch%10 == 0:
          print("\ninput:      ",tokinizer.decode(input[0]))
          print("\ntarget:     ",tokinizer.decode(target[0]))
          print("\nprediction: ",tokinizer.decode(prediction[0].argmax(-1)))
          print(f"input:{input.dtype}, target:{target.dtype}, prediction:{prediction.dtype}")
          print(f"input: requires_grad={input.requires_grad}, target: requires_grad={target.requires_grad}, prediction: requires_grad={prediction.requires_grad}")
          print(f"input: grad={input.grad}, target: grad={target.grad}, prediction: grad={prediction.grad}")
          for name, param in self.model.named_parameters():
              print(f'{name}: requires_grad={param.requires_grad}, dtype={param.dtype}, grad={param.grad.shape}')

        yield test_loss,test_acc

"""###Start Training"""

EPOCHS=21
pretrain=None

if pretrain=="encoder":
  TrainEncoder(EPOCHS=EPOCHS,model=encoder,loss=loss,device=device,savePath="preTrainedEncoder_2",tokinizer=tokinizer,csvFilePath="/content/drive/MyDrive/preTrainingData.csv",trainSplit=.8)
else:
  TrainNN(EPOCHS=EPOCHS,model=seq2seq,loss=loss,device=device,savePath="trainedSeq2seq_2",tokinizer=tokinizer,csvFilePath="/content/drive/MyDrive/shitpostCommentData.csv",trainSplit=.8)